{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>pixel10</th>\n",
       "      <th>pixel11</th>\n",
       "      <th>pixel12</th>\n",
       "      <th>pixel13</th>\n",
       "      <th>pixel14</th>\n",
       "      <th>pixel15</th>\n",
       "      <th>pixel16</th>\n",
       "      <th>pixel17</th>\n",
       "      <th>pixel18</th>\n",
       "      <th>pixel19</th>\n",
       "      <th>pixel20</th>\n",
       "      <th>pixel21</th>\n",
       "      <th>pixel22</th>\n",
       "      <th>pixel23</th>\n",
       "      <th>pixel24</th>\n",
       "      <th>pixel25</th>\n",
       "      <th>pixel26</th>\n",
       "      <th>pixel27</th>\n",
       "      <th>pixel28</th>\n",
       "      <th>pixel29</th>\n",
       "      <th>pixel30</th>\n",
       "      <th>pixel31</th>\n",
       "      <th>pixel32</th>\n",
       "      <th>pixel33</th>\n",
       "      <th>pixel34</th>\n",
       "      <th>pixel35</th>\n",
       "      <th>pixel36</th>\n",
       "      <th>pixel37</th>\n",
       "      <th>pixel38</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel744</th>\n",
       "      <th>pixel745</th>\n",
       "      <th>pixel746</th>\n",
       "      <th>pixel747</th>\n",
       "      <th>pixel748</th>\n",
       "      <th>pixel749</th>\n",
       "      <th>pixel750</th>\n",
       "      <th>pixel751</th>\n",
       "      <th>pixel752</th>\n",
       "      <th>pixel753</th>\n",
       "      <th>pixel754</th>\n",
       "      <th>pixel755</th>\n",
       "      <th>pixel756</th>\n",
       "      <th>pixel757</th>\n",
       "      <th>pixel758</th>\n",
       "      <th>pixel759</th>\n",
       "      <th>pixel760</th>\n",
       "      <th>pixel761</th>\n",
       "      <th>pixel762</th>\n",
       "      <th>pixel763</th>\n",
       "      <th>pixel764</th>\n",
       "      <th>pixel765</th>\n",
       "      <th>pixel766</th>\n",
       "      <th>pixel767</th>\n",
       "      <th>pixel768</th>\n",
       "      <th>pixel769</th>\n",
       "      <th>pixel770</th>\n",
       "      <th>pixel771</th>\n",
       "      <th>pixel772</th>\n",
       "      <th>pixel773</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6792</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
       "6792      0       0       0       0  ...         0         0         0         0\n",
       "\n",
       "[1 rows x 785 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train['label']\n",
    "X_train = train.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Check for null and missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for images with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count       784\n",
       "unique        1\n",
       "top       False\n",
       "freq        784\n",
       "dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values in the train and test dataset so we can go ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "test = test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    4. Reshape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values.reshape(-1, 28, 28, 1)\n",
    "test = test.values.reshape(-1, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test images (28px x 28px) have been stock into pandas dataframe as 1D vector of 784 values. We need to reshape the data to 28 x 28 x 1 3D matrices.\n",
    "\n",
    "Keras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so they use only one channel. For RGB images, there are 3 channels so we would have reshaped 784px to 28 x 28 x 3 3D matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    5. Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode labels to one hot vectors (ex: 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    4\n",
       "4    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "Y_train = to_categorical(Y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    6. Split training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x272def4b490>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[2][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1. Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model.add(Conv2D(...))** adds a convolutional layer to the model. Convolution is a fundamental operation in image processing that extracts local features.\n",
    "\n",
    "- **filters=32** indicates the layer will have 32 filters (also known as kernels). Each filter learns to detect different patterns in the input.\n",
    "\n",
    "- **kernel_size=(5,5)** specifies the size of the convolutional kernel or filter. In this case, the kernel is a 5x5 matrix that slides over the input to perform convolution.\n",
    "\n",
    "- **padding='Same'** controls how to handle the image border. 'Same' means the border is padded with zeros to maintain the input size after convolution.\n",
    "\n",
    "- **activation='relu'** indicates the Rectified Linear Unit (ReLU) activation function will be applied after convolution. ReLU is commonly used to introduce non-linearities in the network.\n",
    "\n",
    "- **input_shape=(28,28,1)** specifies the input shape to the layer. In this case, it expects a 28x28-pixel image with a single color channel (grayscale). The '1' in the last dimension denotes a single channel.\n",
    "\n",
    "**model.add(MaxPool2D(pool_size=(2,2)))**\n",
    "\n",
    "- **MaxPool2D** es una capa de reducciÃ³n de muestreo o pooling en una red neuronal convolucional.\n",
    "\n",
    "- **pool_size=(2,2)** specifies the window size for performing max pooling. In this case, it's a 2x2-pixel window. The purpose of this layer is to reduce the spatial size of the representation, thereby reducing the number of parameters and computations in the network while preserving important features learned by the convolutional layers. Max pooling takes the maximum value from each window of the image and retains it, discarding the rest.\n",
    "\n",
    "**model.add(Dropout(0.25))**\n",
    "\n",
    "- **Dropout** is a regularization technique used to reduce overfitting in neural network models.\n",
    "\n",
    "- **0.25** indicates the fraction of units in the network that will be randomly deactivated during training. During training, Dropout randomly deactivates a fraction of the neurons from the preceding layer, forcing the network to learn features in a more robust manner, preventing it from becoming too specialized in certain features of the training data. This helps improve the model's generalization to new data and reduces the risk of overfitting.\n",
    "\n",
    "#### **Lines of code typically found towards the end of a Convolutional Neural Network (CNN) architecture**\n",
    "\n",
    "**model.add(Flatten()):** The Flatten layer is used to convert the 2D feature maps obtained from the convolutional layers into a 1D vector. This is necessary because the subsequent layers (Dense layers) expect a one-dimensional input.\n",
    "\n",
    "**model.add(Dense(256, activation = \"relu\")):** The Dense layer with 256 units (neurons) and ReLU activation takes the flattened 1D vector and applies a linear operation to produce a set of outputs. The ReLU activation introduces non-linearity to the network.\n",
    "\n",
    "**model.add(Dropout(0.5)):** Used after the Dense layer to regularize the network  by randomly dropping out a fraction (here, 50%) of the units/neurons during training, which helps prevent overfitting.\n",
    "\n",
    "**model.add(Dense(10, activation = \"softmax\")):** This last Dense layer has 10 units, corresponding to the number of classes in the classification task. The softmax activation function is used here, which normalizes the outputs for each class into probability-like values, allowing the model to predict the probability that an input belongs to each class. The class with the highest probability is chosen as the final prediciton.\n",
    "\n",
    "This architecture's purpose is to transition from the feature extraction done by the convolutional layers to a format suitable for classification. The Dense layers act as a classifier, taking the learned features and making predicitons. The final Softmax layer provides probabilities for each class, enabling the model to make predicitons about the input data's class membership probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Conv2D(filters=32, kernel_size=(5,5), padding='Same', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    2. Set optimizer and annealer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the optimizer and using  an annealing technique are common practices in training neural networks, especially in deep learning models.\n",
    "\n",
    "**Optimizer:** Responsible for updating the weights of the neural network during training in order to minimize the loss function. It implements optimization algorithms such as Stochastic Gradient Descent (SGD), Adam, RMSprop, etc. Each optimizer has its own characteristics and update rules.\n",
    "\n",
    "**Learning Rate Annealing:** Annealing refers to the process of gradually reducing the learning rate during training. It can help the optimization processby allowing the model to converge faster and potentially reach a better optimum. Learning rate annealing is used to prevent the model from overshooting the minimum and to refine the learning process by decreasing the learning rate over time.\n",
    "\n",
    "Combining an optimizer with a learning rate annealing strategy allows for more efficient training by optimizing the model's convergence and allowing it to reach a better solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import RMSprop\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = RMSprop(learning_rate=0.001, rho=0.9, epsilon=1e-08)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a learning rate annealer\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=1e-4)\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=3, verbose=1, factor=0.5, min_lr=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 16\n",
    "batch_size = 86"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    3. Data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ImageDataGenerator** is a powerful tool in Keras used for real-time data augmentation during model training, primarily with image data. It allows you to generate bacthes of augmented data in real-time, on-the-fly, as the model is being trained.\n",
    "\n",
    "Some key functionalities:\n",
    "- **Data Augmentation:** It performs real-time data augmentation on images by applying various tranformations like rotations, shifts, flips, zooms, shear, and more. Data augmentation helps increase the diversity of the training dataset by creating modified versions of the images. This aids improving the model's ability to generalize and reduces overfitting.\n",
    "- **Batch Processing:** It generated batches of augmented data. These batches can be directly fed into the model for training using the **`flow_from directory`** or **`flow`** methods.\n",
    "- **Normalization and Preprocessing:** **`ImageDataGenrator`** can perform data preprocessing and normalization on the fly. For instance, it can rescale pixel values to a certain range (0 to 1) or substract the mean and divide by the standard deviation.\n",
    "- **Usage with Model Training:** It integrates seamlessly with the **`fit_generator`** method in Keras, allowing you to use augmented data directly within the model training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,\n",
    "    samplewise_center=False,\n",
    "    featurewise_std_normalization=False,\n",
    "    samplewise_std_normalization=False,\n",
    "    zca_whitening=False,\n",
    "    rotation_range=15,\n",
    "    zoom_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    width_shift_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "datagen.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noemi\\AppData\\Local\\Temp\\ipykernel_17304\\1028432422.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439/439 [==============================] - ETA: 0s - loss: 0.4281 - accuracy: 0.8616WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 33ms/step - loss: 0.4281 - accuracy: 0.8616 - val_loss: 0.0797 - val_accuracy: 0.9738 - lr: 0.0010\n",
      "Epoch 2/16\n",
      "438/439 [============================>.] - ETA: 0s - loss: 0.1389 - accuracy: 0.9576WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 33ms/step - loss: 0.1388 - accuracy: 0.9576 - val_loss: 0.0527 - val_accuracy: 0.9855 - lr: 0.0010\n",
      "Epoch 3/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.1053 - accuracy: 0.9684WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 33ms/step - loss: 0.1053 - accuracy: 0.9684 - val_loss: 0.0375 - val_accuracy: 0.9886 - lr: 0.0010\n",
      "Epoch 4/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9736WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 34ms/step - loss: 0.0893 - accuracy: 0.9736 - val_loss: 0.0265 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 5/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.0774 - accuracy: 0.9774WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 34ms/step - loss: 0.0774 - accuracy: 0.9774 - val_loss: 0.0283 - val_accuracy: 0.9905 - lr: 0.0010\n",
      "Epoch 6/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.0656 - accuracy: 0.9804WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 15s 34ms/step - loss: 0.0656 - accuracy: 0.9804 - val_loss: 0.0414 - val_accuracy: 0.9893 - lr: 0.0010\n",
      "Epoch 7/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.0682 - accuracy: 0.9793WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 16s 37ms/step - loss: 0.0682 - accuracy: 0.9793 - val_loss: 0.0293 - val_accuracy: 0.9912 - lr: 0.0010\n",
      "Epoch 8/16\n",
      "439/439 [==============================] - ETA: 0s - loss: 0.0637 - accuracy: 0.9812WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_acc` which is not available. Available metrics are: loss,accuracy,val_loss,val_accuracy,lr\n",
      "439/439 [==============================] - 20s 45ms/step - loss: 0.0637 - accuracy: 0.9812 - val_loss: 0.0269 - val_accuracy: 0.9929 - lr: 0.0010\n",
      "Epoch 9/16\n",
      "235/439 [===============>..............] - ETA: 8s - loss: 0.0642 - accuracy: 0.9812"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                              epochs=epochs, validation_data=(X_val, Y_val),\n",
    "                              verbose=1, steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                              callbacks=[early_stop, learning_rate_reduction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Save trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('../weights/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load trained model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('../weights/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "875/875 [==============================] - 5s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the index with the maximum probability\n",
    "results = np.argmax(results,axis = 1)\n",
    "results = pd.Series(results,name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "submission.to_csv(\"../submission/cnn_mnist_datagen.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ironhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
